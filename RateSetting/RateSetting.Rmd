---
title: "Rate Setting"
author: "James Woods"
date: "3/3/2017"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Overview

+ The idea is to create a tariff a rate structure that,
    + Will satisfy the revenue requirements until the next rate case.
    + Balances efficiency, in the dead weight loss sense, with other objectives, e.g., equity.
    + Includes at least a nod to marginal cost pricing at some level.

## General Rate Patterns

+ Two-Part
+ Decreasing block tariff
+ Increasing block tariff
+ Peak/TOU pricing
+ Real-Time

The big question?  What do you mean by marginal cost.

## Variations for Other Purposes

+ Life-line or low-income
+ Electric heat or electric vehicle

## Data Overview

+ Appliance surveys for bottom up
+ System and rate class use time series for top down

Both are used to create models of future consumer behavior and forecast how the consumers will react to rates so that you can verify if the revenue requirement is satisfied.

## But First, Lets Talk Weather

How you model weather depends on the frequency of the load data you are working with.

+ Weather data is commonly from NOAA and it was a pain to get with FTP servers.
+ I used to have a cron job to get new California data every month and every day.
+ NOAA has better APIs these days.  For example to pull daily temperature data for PDX is 

> meteo_pull_monitors("USW00024229", date_min = "2016-01-01", date_max = "2016-12-31")

+ A Year of hourly data is 

> isd(usaf="726980", wban="24229", year =2016)



```{r, message=FALSE, warning=FALSE, include=FALSE}
library(rnoaa)


PortlandWeather <- meteo_pull_monitors("USW00024229",
                                      date_min = "2016-01-01",
                                     date_max = "2016-12-31") 
stations <- isd_stations()
PortlandHrly <- isd(usaf="726980", wban="24229", year =2016)

```

## In General

+ Try to get weather data that has similar frequency as your data.
    + If you are working with hourly load data, get hourly weather data.
    + If you are working with daily or larger aggregations of use, get daily weather and aggregate up.
+ Working with hourly data can be hard
    + Not all, and not a majority, of the NOAA data will have hourly observations.
    + Daily summaries are more common and you can get closer geographic matches.
    
+ Make trade-offs
    + You have fewer 1st order hourly stations (http://www.weather2000.com/1st_order_wbans.txt).
    + Often does not cover all the territory.
    + You sometimes have to make up your own data for isolated places using nearby 1st order stations.

## What NOAA gives you

Metric!!

Core Values (https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)

+ prcp: Precipitation, in tenths of mm
+ tavg: Average temperature, in tenths of degrees Celsius
+ tmax: Maximum temperature, in tenths of degrees Celsius
+ tmin: Minimum temperature, in tenths of degrees Celsius
+ awnd: Average daily wind speed, in meters / second
+ wsfg: Peak gust wind speed, in meters / second

There are variables about gusts, clouds and other weather.  Except for dew point, which is used for relative humidity, I have stayed away from the others.

## Warning about hourly weather

+ These are mostly airports
+ Higher wind
+ Fewer trees
+ They tend to be cooler than the non-airport.  Keep in mind if you use a mix of hourly and daily measurements.

## Using Hourly Weather with Hourly Load

+ Not as easy as just putting in the temperature
+ Load for systems and individual households have an asymmetric response to temperatures.
+ The load tends to increase with temperature when temperatures are high.
+ Loads increase as temperature falls with low temperatures.
+ The cut between low and high temperature is different by region and household and often called the set-point or base temperature.
+ If you have a specification that allows for a an asymmetric, U-shaped or V-shaped response, you should be OK.

## Typical Shape

+ V shaped with an apex at around 68F (20C).
+ Slope of load reaction to high temperatures about x3 the reaction to low temperatures.
+ The low temperature load is either electric heat or air handlers.
+ Techniques
    + Run model multiple times with different set-point and choose the one with maximum fit.
    + Estimate the set-point within the model by making the transition U-shaped rather than V-shaped.
    + Discretize temperature into 5F blocks and estimate effect of each temperature.
    
## The techniques (Scan Over Setpoints)

+ Set up a load equation with 
$$L_i = \alpha + \beta_1 (b-T_i)I(b-T_i>0) + \beta_2(T_i -b)I(T_i -b>0) + g(X_i)$$
    
    + $b$ is the set-point
    + $T$ is the temperature
    + I is an indicator function
    + $g(X_i)$ are the other variables
    
+ Run regression over a range of b values and pick the one with the highest goodness of fit.

## What is Wrong With This? 

+ Everything !!!!
+ Treats the unknown $b-T$ data as known
+ Estimates $b$ outside the model even though it is used in the model and correlated with other parameters.
+ Don't get me started on multiple comparisons.

How to fix this, pick a base temp without looking and stick with it.  Easier and only has an errors-in-variables problem. 


## Smooth or Discretitze

+ Loess, a non-parametric technique would work for this problem.
+ Polynomial of degree 3 or higher, e.g., $\beta_1 T + \beta_2 T^2 + \beta_3 T^3$
+ Discretize temperature
    + Create a dummy variable for temperatures in the, [30,35), [35,40) range and so on.
    + Similar to polynomial and loess in flexibility.
    + Does not guarantee monotonic response without restrictions.


## How Many Models

+ It is possible for you to have one model, relation between weather and load, per hour per day of week, 24*7=168.
+ More than likely you can aggregate some hours, many night time hours can be aggregated.
+ Many days can be treated as the same:
    + Sometimes Weekday, Weekend
    + M-Th, F, Weekend.
    + M, F, T-Th, Weekend.
    + Some states have different Sunday shapes because of blue laws
+ The models will be part of a system
    + Often previous hour and previous day are useful.
    + Often errors within a day have correlated errors, SUR models

## Dealing with Daily and Lower Frequency Data

+ You often have to use daily data, because that is the best or only weather station.
+ You are modeling daily peak load.
+ Don't try averaging over longer periods.
+ You are in the land of degree days.

## What is a degree day?

+ Stole it from agriculture, growing degree day.
+ $HDD(b) = \max(0, b-Avg)$
+ $CDD(b) = \max(0, Avg-b)$
+ Average Temperature is the average of the daily high and daily low.
    + Not the average of the hours.
    + With shared base, there are either HDD in a day or CDD but not both.
    
## The Logic of HDD and CDD

+ Really tries to get at heating and cooling load.
+ Not comparable if the diurnal swing is very different, Hawaii vs Mojave.
+ Not knowing b has the same trouble as before.
+ Errors in CDD because of unknown $b$ cause correlated errors in HDD. (More Later)
+ I have tried way more complicated things and XDD measures work.

## Typical Models

+ $kWh = \alpha + \beta_1 CDD + \beta_2 HDD + g(X)$
+ CDD and HDD often, but not always have the same base.
+ There is a scanning methodology for the base and a within model technique


## Problems with CDD and HDD as commonly used

+ People make econometrics mistakes that make their models look better.
+ They treat XDD and the base temperature as a known instead of something that should be estimated.
+ Upshot:
    + Biased effect of HDD and CDD parameter, one up and one down.
    + Variance of estimators are smaller than they should be.
+ Use the spline based method, but watch for the point discontinuity.

http://www.sciencedirect.com.proxy.lib.pdx.edu/science/article/pii/S014098831400139X


## Main Points of the Paper

+ Stop guessing set-points and stop scanning for set-points
+ Either way, you are going to cause persistent under and over prediction for weather driven use.
+ Errors-in-variables 
    + Simple with one variable, just attenuation effect.
    + With two variables that are correlated, the attenuation pushes off each other biasing one value up and the other down.
    + It _tends_ to increase apparent precision of estimates.

## Heat Buildup and Other Effects

+ Loads at the beginning of a heat wave are typically lower than loads later even when the temperature is the same.
+ Happens when there is not enough time to let things cool off.
    + Think hot concrete
    + Attic spaces
+ You often put in a lagged CDD and HDD in your model $kWh = \alpha + \beta_1 CDD + \beta_2 HDD  + \beta_3 L(CDD) + g(X)$
+ or even polynomial lags $kWh = \alpha + \beta_1 CDD + \beta_2 HDD + \beta_3 \sum \gamma^n L^n HDD +  g(X)$ and estimate the weight of the lag, $\gamma$. 

## For the Metrics Crowd  

There is a lot of autocorrelation or a lot of misspecified models or a lot of people that want to avoid talking about autocorrelation.

+ Often misspecification looks like autocorrelation.
+ Once people learn that, they correct for autocorrelation they forget.

## Example of False Autocorrelation

```{r, echo=FALSE, message=FALSE, warning=FALSE}
FakeAuto <- data.frame(X = rnorm(200 , mean = 50, sd = 15))
FakeAuto$Y <- (FakeAuto$X-50)^2 + rnorm(200, sd = 100)

library(ggplot2)
library(dplyr)
FakeAuto %>% ggplot(aes(X,Y)) + geom_point()+ geom_smooth(method='lm')

```

## Why?

+ Simple specifications are easy to use
    + "How much does this go up by with a 2C increase in average temp?"
    + "What is the elasticity?" 
+ People know how to fix autocorrelation.
+ Often people don't fix autocorrelation because it hurts their argument.
    + Correcting for autocorrelation reduces the precision of estimates.
    + Turns p > .95 into p >.7 pretty quickly.
+ Non-technical people often have unreasonable standards or rules of thumb for econometric models.  
    + They treat low significance as the fault of the analyst.
    + All the tricks in the world can't fix a weak relationship and noisy data.

## Weather Normalization

There are two distinct weather normalization techniques in use.

+ First Meaning and correct use
    + Form a statistical model of load or energy use that depends on weather $y=f(Weather, X) + \epsilon$
    + Estimate the model, $\hat f(Weather, X)$
    + Use the average weather, commonly some kind of 30-year average to forecast what load or use would be for a typical weather year.
    + $\hat y = \hat f (\bar{Weather}, \bar X)$
    + This is a simple regression based forecast.

## Why do this?

+ If you hold equipment constant, energy use is a combination of weather and behavior.
+ You may have behavior that reduces use but it could be overwhelmed by weather, example Winter 16-17.
+ Allows you to hold weather constant


## The second, bad way

Use the normalized load and weather in a second model ...

+ Remember how "estimate" means different things to engineers and statisticians or economists?
+ They use normalized values as exogenous, i.e., RHS,  variables in other models.
    + These have lower variance, if original model was correct, than the actual variables.
    + Is kind of like 2SLS without the variance correction.
+ They use normalized values as endogenous, i.e., LHS, variables.
    + This is like trying to construct a model that is a good fit for the forecasts of a different model.
    + Artificially more precise estimates.

https://drive.google.com/file/d/0B2gPTwcA567qaF9pX0YwSkRqVlk/view?usp=sharing


## A Few Notes On Peak Esimation

There are a few common methodologies for estimating peak load.

+ *Peak Data* :Simple regression of monthly peaks on exogenous variables, i.e., just restrict the data.
+ *Simulate* :All data used in a load model but when forecasting bootstrap the error and find the distribution of the maximum load in year.
+ *Quantile regression*

## Peak Data

+ Most common technique
+ Last seen in an IRP
+ Most common problem is that it assumes that the error terms are symmetric.
    + Remember that the peak load is not just extreme values for exogenous variables
    + High values for the error term.
+ Probably should be estimated with a frontier but with negative skew --  not positive skew.

## Simulate

+ Much less common
+ Model all loads.
+ Forecast future values
+ Bootstrap the errors from the original model and add to forecasts.
    + Means drawing, with replacement, a new set of error terms from the one you have.
    + Add these error terms, which have mean zero, to get a new forecast.
    + Find the maximum value in a year.
    + Repeat to get the sampling distribution for the maximum value in a year.
    
## Simulate (Cont)

+ This is how I did it until quantile regression became practical in about 2005.
+ BTW Bootstrapping is a great technique to learn.

"When the math gets too hard, bootstrap."

## Quantile Regression

+ Preferred tool but has not hit the PUC level yet.
+ Idea
    + OLS finds a line that has half the probability above and below the line.
    + Quantile allows you to choose how much probability mass is above and below the line.
+ You can literally say, given known weather, we expect the load to be below x, 90% of the time.
+ No assumption that the quantile lines are parallel

## Bottom Up Estimates

+ This usually uses survey data
    + RECS https://www.eia.gov/consumption/residential/
    + CBECS https://www.eia.gov/consumption/commercial/
    + MECS https://www.eia.gov/consumption/manufacturing/
+ State Do it Too
    + California RASS
    + Oregon does a few of these


## Pattern

+ Survey http://www.energy.ca.gov/2010publications/CEC-200-2010-004/CEC-200-2010-004-AP.PDF (Page 7)
+ Engineers take the data and form Unit Energy Consumption (UEC) estimates by specific end uses.
    + These UECs are usually annual kWh (3412 BTU) measures but you will also see them combined with Therms (100K BTU) for gas and BTU for total energy use.
    + Lots of assumptions need to be made for these estimates.
+ These estimates are then compared to statistical estimates of actual use, or
    + NAC (Normalized Annual Consumption), the adjustment I warned you about.
    + They create a statistically adjusted engineering model (SAE)
+ Everyone gets to use these numbers to estimate use given estimates of changes in the number of consumers.

## Problems? Plenty

+ endogeneity
    + The choice of tenure (Own/Rent) and structure type is not independent of race and ethnicity but treated like it it.
    + Conference paper with a student https://github.com/woodsjam/EnergyEthnicity/blob/master/ConferencePaper/DraftEdwardsWoods.pdf
    + Housing market discrimination probably has something to do with use patterns.

+ multicollinearity
    + Explanatory variables are correlated.
    + They share explanatory power
    + Messes up parameter estimates and variances are too high but forecasts are still good.
    + https://s3.amazonaws.com/woodsjworking/DraftConferncePaper.pdf (Pages 18-19)

##  More Problems

+ Statistically Adjusted Engineering Models
    + Predict a real value with an engineering estimate and an intercept.
    + $Real = \alpha + \beta Estimate + \epsilon$
    + If the engineering estimate is good, $\alpha=0$ and $\beta=1$.
    + Didn't work for California http://www.energy.ca.gov/2010publications/CEC-200-2010-004/CEC-200-2010-004-V1.PDF (Page 129 or 143)
    + Better than previous RASS when they didn't notice that there was a problem.

## Tariff Background

+ The ideal is to charge the marginal cost.
    + Efficient use
    + Minimizes dead weight loss
+ Marginal cost is different:
    + Every moment of every day of the year
    + Every customer
+ Tariffs, except for real-time tariffs, lump these moments together and treat them as if they are the same.
+ Tariffs need to be designed as close to marginal cost as possible while:
    + Being easy to understand and predict for consumer
    + Allowing for collection of the revenue requirement

## Why Fixed Rates Cause a Problem

+ With fixed rate
+ When low demand, there is under use and DWL
+ When high demand, there is over use and DWL

## Real-Time Tariff

+ Fixed cost per month
+ Marginal cost of electricity changes hourly
    + Usually by day-ahead wholesale market -- so you have warning.
    + Caps may be in place.
+ Most common in Industrial and Commercial but rare in Residential
    + You see them when there is greater capacity to react.
    + When the demands are more elastic
    + When they have generation capability
+ If you have an inelastic demand, the suppler has power

## RT Tariff in the real world

+ Attractive to risk tolerant, higher elasticity consumers
+ Residential consumers do peak shave, 2% or so is a normal estimation.
+ Don't time shift -- in the US.
    + Using timers is not part of US culture.
    + Not built into US equipment
+ It is like prices per kWh doesn't matter but price per month does.

https://www-sciencedirect-com.proxy.lib.pdx.edu/science/article/pii/S092876551100042X

## Peak/TOU 

+ More predictable than RT
+ Still tries to collect attributable cost in each timer period
    + This is probably a mistake if you want shifting and not just peak reduction.
+ DWL is smaller.
+ Peaks are still shaved
+ Smart thermostats will pre-cool and heat. (https://nest.com/support/article/What-happens-during-a-Summer-or-Winter-Rush-Hour)

## Block Tariffs

+ Block tariffs, different prices depending on use are the most common in the US.
+ DBT
    + Tries to mimic scale effects, lower AC as quantity increases.
    + As if all loads were correlated with each other
+ IBT
    + Tries to encourage conservation
    + Often intended to help the poor.

Please keep in mind that this is all price discrimination, either second or third degree.

## IBT

+ High CS for low use.
+ Fixed cost collected at higher use.
+ Intended to help the poor?
    + What if there is an what if larger families have lower income?
    + https://censusreporter.org/data/table/?table=B19019&geo_ids=04000US41&primary_geo_id=04000US41

## DBT

+ CS is more even over use
+ Fixed cost collected at lower use

## Life-line or Low-Income Tarrifs

+ The "on average" fix of IBT is only partially right.
+ We tend to avoid direct income support in the US
+ Lower prices for the poor is an acceptable second-best solution.
    + We do it all the time.
    + Rents, fuel, education, ..., even the Romans did it.
+ May not increase total CS but
    + CS assumes a that each person is the same.
    + All gains to one person is the same as equal gains for all.
    + Lower price for poor, weights them more highly.
+ Wait, why lower prices for students and the elderly?
    + More elastic demand because more flexibility.